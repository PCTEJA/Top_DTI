{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dbc88a8-3168-4e88-969a-5a975784ec7e",
   "metadata": {},
   "source": [
    "# Protein Target Embeddings with ProtT5 LLM\n",
    "\n",
    "#### We utilized the ProtT5 LLM to extract sequence-based features from protein sequences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b2f472-7bf3-43c9-a0be-3a12bf7b8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ProteinEmbeddingsExtractor:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_name = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name, do_lower_case=False, legacy=True)\n",
    "        self.model = T5EncoderModel.from_pretrained(self.model_name).to(self.device).eval()\n",
    "\n",
    "    #generates embeddings for protein sequences\n",
    "    def get_embeddings(self, seq):\n",
    "        sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq)))]\n",
    "        ids = self.tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "        input_ids = torch.tensor(ids['input_ids']).to(self.device)\n",
    "        attention_mask = torch.tensor(ids['attention_mask']).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding_repr = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        emb_0 = embedding_repr.last_hidden_state[0]\n",
    "        return emb_0.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    def process_and_save(self, data, output_dir, dataset_name):\n",
    "        output_dir = Path(output_dir).resolve()\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #gets unique protein sequences from 'Protein' column of the given dataframe.\n",
    "        unique_sequences = data['Protein'].unique()\n",
    "\n",
    "        sequence_embeddings = []\n",
    "        for seq in tqdm(unique_sequences, desc=f\"Processing sequences in {dataset_name}\"):\n",
    "            embedding = self.get_embeddings(seq)\n",
    "            sequence_embeddings.append(embedding)\n",
    "\n",
    "        embeddings_array = np.array(sequence_embeddings)\n",
    "        np.save(output_dir / f\"{dataset_name}_target_sequences.npy\", unique_sequences)\n",
    "        np.save(output_dir / f\"{dataset_name}_sequence_embeddings.npy\", embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6fafa1-cc2f-4dba-8e63-b768cb8a8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base directory\n",
    "base_dir = Path('/bozdagpool/tp0626.unt.ad.unt.edu/Top_DTI_Work/Top_DTI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd9e49-aa5a-41e9-9934-a09295478ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4216c2dadc46ee8527cbb473c311e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44aada09c1654b72afe49ec6d41c122d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/238k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642fd186d05143fd9a4bcb178bd27341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd989062b0544a0eae84e6d459dc95c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1180c10cb3bc466e8f0444bcf87c9226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:629\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    627\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 629\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m data_path \u001b[38;5;241m=\u001b[39m data_path \u001b[38;5;241m=\u001b[39m base_dir  \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m task_paths \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiosnap_random\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiosnap/random\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_random\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman/random\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_cold\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman/cold\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 9\u001b[0m protein_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mProteinEmbeddingsExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m all_datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Process each dataset\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mProteinEmbeddingsExtractor.__init__\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRostlab/prot_t5_xl_half_uniref50-enc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, legacy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mT5EncoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4900\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4891\u001b[0m     gguf_file\n\u001b[1;32m   4892\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4893\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4894\u001b[0m ):\n\u001b[1;32m   4895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4898\u001b[0m     )\n\u001b[0;32m-> 4900\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4907\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4920\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4921\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1066\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[0;32m-> 1066\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   1072\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   1073\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[1;32m   1074\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[1;32m   1075\u001b[0m     )\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         snapshot_download(\n\u001b[1;32m    495\u001b[0m             path_or_repo_id,\n\u001b[1;32m    496\u001b[0m             allow_patterns\u001b[38;5;241m=\u001b[39mfull_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    506\u001b[0m         )\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    992\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1171\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1171\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1184\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1723\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[1;32m   1722\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1723\u001b[0m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n",
      "File \u001b[0;32m~/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:629\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m    627\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 629\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_path = data_path = base_dir  / 'datasets'\n",
    "\n",
    "task_paths = {\n",
    "    \"biosnap_random\": data_path / \"biosnap/random\",\n",
    "    \"human_random\": data_path / \"human/random\",\n",
    "    \"human_cold\": data_path / \"human/cold\"}\n",
    "    \n",
    "\n",
    "protein_extractor = ProteinEmbeddingsExtractor()\n",
    "all_datasets = {}\n",
    "\n",
    "# Process each dataset\n",
    "for dataset_name, dataset_path in task_paths.items():\n",
    "    data_dir = Path(dataset_path)\n",
    "\n",
    "   \n",
    "    train_file = data_dir / 'train.csv'\n",
    "    val_file = data_dir / 'val.csv'\n",
    "    test_file = data_dir / 'test.csv'\n",
    "    \n",
    "    if train_file.exists() and val_file.exists() and test_file.exists():\n",
    "        \n",
    "        train_data = pd.read_csv(train_file)\n",
    "        val_data = pd.read_csv(val_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "        \n",
    "      \n",
    "        full_data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "\n",
    "        all_datasets[dataset_name] = full_data\n",
    "\n",
    "        \n",
    "        #output directory to save embeddings\n",
    "        out_dir = base_dir  / 'embeddings' /'llm'/ dataset_name / 'target'\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)   \n",
    "        \n",
    "        protein_extractor.process_and_save(full_data, out_dir, dataset_name=dataset_name)\n",
    "        \n",
    "        print(f\"{dataset_name} dataset loaded successfully. Total rows: {len(full_data)}\")\n",
    "    else:\n",
    "        print(f\"Skipping {dataset_name}: train.csv, val.csv, or test.csv not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0915ce03-2971-47a0-a611-ac3b7af1fb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "      <th>protein_llm_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MGDHAWSFLKDFLAGGVAAAVSKTAVAPIERVKLLLQVQHASKQIS...</td>\n",
       "      <td>[0.040794070810079575, 0.1398317515850067, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MVLDLDLFRVDKGGDPALIRETQEKRFKDPGLVDQLVKADSEWRRC...</td>\n",
       "      <td>[0.07856228947639465, 0.09228259325027466, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MGNLKSVAQEPGPPCGLGLGLGLGLCGKQGPATPAPEPSRAPASLL...</td>\n",
       "      <td>[0.030257243663072586, 0.09058675915002823, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MGNAAAAKKGSEQESVKEFLAKAKEDFLKKWESPAQNTAHLDQFER...</td>\n",
       "      <td>[0.07570360600948334, 0.11278703063726425, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MVNENTRMYIPEENHQGSNYGSPRPAHANMNANAAAGLAPEHIPTP...</td>\n",
       "      <td>[0.07552585750818253, 0.09334281831979752, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sequences  \\\n",
       "0  MGDHAWSFLKDFLAGGVAAAVSKTAVAPIERVKLLLQVQHASKQIS...   \n",
       "1  MVLDLDLFRVDKGGDPALIRETQEKRFKDPGLVDQLVKADSEWRRC...   \n",
       "2  MGNLKSVAQEPGPPCGLGLGLGLGLCGKQGPATPAPEPSRAPASLL...   \n",
       "3  MGNAAAAKKGSEQESVKEFLAKAKEDFLKKWESPAQNTAHLDQFER...   \n",
       "4  MVNENTRMYIPEENHQGSNYGSPRPAHANMNANAAAGLAPEHIPTP...   \n",
       "\n",
       "                              protein_llm_embeddings  \n",
       "0  [0.040794070810079575, 0.1398317515850067, -0....  \n",
       "1  [0.07856228947639465, 0.09228259325027466, 0.0...  \n",
       "2  [0.030257243663072586, 0.09058675915002823, 0....  \n",
       "3  [0.07570360600948334, 0.11278703063726425, 0.0...  \n",
       "4  [0.07552585750818253, 0.09334281831979752, 0.0...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the generated target embeddings for the BioSNAP random dataset \n",
    "biosnap_llm_embeddings_path = base_dir / f\"embeddings/llm/biosnap_random\"\n",
    "sequences_names = np.load(biosnap_llm_embeddings_path / f\"target/biosnap_random_target_sequences.npy\", allow_pickle=True)\n",
    "gene_embeddings = np.load(biosnap_llm_embeddings_path / f\"target/biosnap_random_sequence_embeddings.npy\", allow_pickle=True)\n",
    "biosnap_protein_llm = pd.DataFrame({'sequences': sequences_names, 'protein_llm_embeddings': gene_embeddings.tolist() })\n",
    "biosnap_protein_llm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd3d3f-11f7-4fc5-9aae-927bc86ead4d",
   "metadata": {},
   "source": [
    "# Drug Embeddings with MoLFormer LLM\n",
    "\n",
    "### We used the MoLFormer LLM to produce drug representations from chemical SMILES strings.\n",
    "\n",
    "Clone https://github.com/IBM/molformer and change directory to molformer folder\n",
    "\n",
    "MolFormer requires the installation of 'apex.' However, we uninstalled 'apex' after creating drug embeddings due to a compatibility issue with PyTorch.\n",
    "\n",
    "After cloning https://github.com/NVIDIA/apex\n",
    "\n",
    "pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
    "\n",
    "pip install -v --disable-pip-version-check --no-build-isolation --no-cache-dir ./\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae227c-246e-42b7-9a31-e5327818cf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35be9bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "# Set the base directory\n",
    "base_dir = Path('/bozdagpool/tp0626.unt.ad.unt.edu/Top_DTI_Work/Top_DTI')\n",
    "os.chdir(base_dir /'molformer')\n",
    "import builtins\n",
    "if not hasattr(builtins, \"basestring\"):\n",
    "    basestring = str\n",
    "    builtins.basestring = basestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a854168-737b-484b-8621-d567f7ebcc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "CWD = os.getcwd()\n",
    "\n",
    "TRAINING_DIR = os.path.join(CWD, 'molformer', 'training')\n",
    "if TRAINING_DIR not in sys.path:\n",
    "    sys.path.append(TRAINING_DIR)\n",
    "\n",
    "from argparse import Namespace\n",
    "import yaml\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "from training.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from training.train_pubchem_light import LightningModule\n",
    "from rdkit import Chem\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a286d24-ce5c-4eff-807d-bbe4bee5424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.serialization\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import numpy.core.multiarray\n",
    "\n",
    "\n",
    "class MoleculeEmbeddingsExtractor:\n",
    "\n",
    "    def __init__(self, model_path, checkpoint_path):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Loading MolFormer model from: {model_path}\")\n",
    "\n",
    "        # Load configuration\n",
    "        with open(Path(model_path) / 'data/Pretrained MoLFormer/hparams.yaml', 'r') as f:\n",
    "            self.config = Namespace(**yaml.safe_load(f))\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = MolTranBertTokenizer(Path(model_path) / 'bert_vocab.txt')\n",
    "\n",
    "        # --- MANUAL CHECKPOINT LOADING ---\n",
    "\n",
    "        # 1. Create an instance of the model architecture\n",
    "        model = LightningModule(self.config, self.tokenizer.vocab)\n",
    "\n",
    "        # 2. Manually load the checkpoint file with weights_only=False\n",
    "        # This is the step that was failing before.\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)\n",
    "\n",
    "        # 3. Load the model's learned weights ('state_dict') from the checkpoint\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        # 4. Set the model to evaluation mode and move to the correct device\n",
    "        self.model = model.to(self.device).eval()\n",
    "\n",
    "        print(\"Model loaded successfully!\")\n",
    "            \n",
    "\n",
    "    def batch_split(self, data, batch_size=64):  \n",
    "        for i in range(0, len(data), batch_size):\n",
    "            yield data[i:i + batch_size]\n",
    "    \n",
    "    def embed(self, smiles, batch_size=64):\n",
    "        \"\"\"\n",
    "        Embed SMILES strings into molecule embeddings.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        embeddings = []\n",
    "        for batch in self.batch_split(smiles, batch_size=batch_size):\n",
    "            batch_enc = self.tokenizer.batch_encode_plus(batch, padding='longest', add_special_tokens=True)\n",
    "            idx = torch.tensor(batch_enc['input_ids']).to(self.device)\n",
    "            mask = torch.tensor(batch_enc['attention_mask']).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                token_embeddings = self.model.blocks(self.model.tok_emb(idx), length_mask=LM(mask.sum(-1)))\n",
    "\n",
    "            # Average pooling over tokens\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            embedding = sum_embeddings / sum_mask\n",
    "            embeddings.append(embedding.cpu())\n",
    "        return torch.cat(embeddings)\n",
    "\n",
    "  \n",
    "\n",
    "    def canonicalize(self, s):\n",
    "        # Ensure s is not None and is a valid SMILES string\n",
    "        if s is not None and Chem.MolFromSmiles(s):\n",
    "            return Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True, isomericSmiles=False)\n",
    "        else:\n",
    "            return None  # Return None if the SMILES string is invalid or None\n",
    "\n",
    "    \n",
    "    \n",
    "    def process_and_save(self, data, output_dir, dataset_name):\n",
    "         \n",
    "        if 'SMILES' not in data.columns:\n",
    "            raise ValueError(\"Dataset does not contain a 'SMILES' column.\")\n",
    "    \n",
    "        output_dir = Path(output_dir).resolve()\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        # Get unique SMILES\n",
    "        unique_smiles = data['SMILES'].unique()\n",
    "        \n",
    "        # Apply the canonicalization function\n",
    "        canonicalized_smiles = [self.canonicalize(s) for s in unique_smiles]\n",
    "    \n",
    "        print(f\"Extracting embeddings for {len(unique_smiles)} unique SMILES in {dataset_name}\")\n",
    "    \n",
    "        # Filter out invalid canonical SMILES (None)\n",
    "        valid_indices = [i for i, s in enumerate(canonicalized_smiles) if s is not None]\n",
    "        valid_smiles = [unique_smiles[i] for i in valid_indices]\n",
    "        valid_canonical_smiles = [canonicalized_smiles[i] for i in valid_indices]\n",
    "        \n",
    "        # Extract embeddings\n",
    "        if len(valid_canonical_smiles) == 0:\n",
    "            print(\"No valid canonical SMILES found. Skipping embedding extraction.\")\n",
    "            return\n",
    "    \n",
    "        embeddings = self.embed(valid_canonical_smiles).numpy()\n",
    "    \n",
    "        # Double-check: Filter again if embeddings are not generated\n",
    "        if len(valid_canonical_smiles) != len(embeddings):\n",
    "            raise ValueError(\"Mismatch in valid canonical SMILES and embeddings length.\")\n",
    "    \n",
    "        # Save the filtered SMILES, canonical SMILES, and embeddings\n",
    "        np.save(output_dir / f\"{dataset_name}_smiles.npy\", valid_smiles)\n",
    "        np.save(output_dir / f\"{dataset_name}_canonical_smiles.npy\", valid_canonical_smiles)\n",
    "        np.save(output_dir / f\"{dataset_name}_molecule_embeddings.npy\", embeddings)\n",
    "    \n",
    "        print(f\"Saved {len(embeddings)} embeddings.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95bf66c-cda1-4176-8687-4ca50fec20e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MolFormer model from: /bozdagpool/tp0626.unt.ad.unt.edu/Top_DTI_Work/Top_DTI/molformer\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bozdagpool/tp0626.unt.ad.unt.edu/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/seed.py:48: LightningDeprecationWarning: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.seed.seed_everything` instead.\n",
      "  rank_zero_deprecation(\n",
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Model loaded successfully!\n",
      "\n",
      "Processing dataset: biosnap_random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:22:40] Unusual charge on atom 0 number of radical electrons set to zero\n",
      "[18:22:40] Unusual charge on atom 0 number of radical electrons set to zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for 4505 unique SMILES in biosnap_random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bozdagpool/tp0626.unt.ad.unt.edu/Top_DTI_Work/Top_DTI/.venv/lib/python3.10/site-packages/fast_transformers/feature_maps/fourier_features.py:37: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2496.)\n",
      "  Q, _ = torch.qr(block)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4505 embeddings.\n",
      "\n",
      "Processing dataset: human_random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:22:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:45] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for 2726 unique SMILES in human_random\n",
      "Saved 2726 embeddings.\n",
      "\n",
      "Processing dataset: human_cold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:22:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:22:47] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for 1813 unique SMILES in human_cold\n",
      "Saved 1813 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Define model, tokenizer, and checkpoint paths\n",
    "\n",
    "lib_path = Path(base_dir/\"molformer\")\n",
    "checkpoint_path = lib_path / \"data/Pretrained MoLFormer/checkpoints/N-Step-Checkpoint_3_30000.ckpt\"\n",
    "\n",
    "\n",
    "molecule_extractor = MoleculeEmbeddingsExtractor(\n",
    "    model_path=lib_path,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "\n",
    "data_path = data_path = base_dir  / 'datasets'\n",
    "\n",
    "task_paths = {\n",
    "    \"biosnap_random\": data_path / \"biosnap/random\",\n",
    "    \"human_random\": data_path / \"human/random\",\n",
    "    \"human_cold\": data_path / \"human/cold\"}\n",
    "    \n",
    "for dataset_name, dataset_path in task_paths.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    dataset_dir = Path(dataset_path)\n",
    "\n",
    "    train_file = dataset_dir / 'train.csv'\n",
    "    val_file = dataset_dir / 'val.csv'\n",
    "    test_file = dataset_dir / 'test.csv'\n",
    "\n",
    "    if train_file.exists() and val_file.exists() and test_file.exists():\n",
    "       \n",
    "        train_data = pd.read_csv(train_file)\n",
    "        val_data = pd.read_csv(val_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "\n",
    "        full_data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "\n",
    "        # Check for 'SMILES' column and process embeddings\n",
    "        if 'SMILES' in full_data.columns:\n",
    "            output_dir = Path(base_dir /f\"embeddings/tda/{dataset_name}/drug\")\n",
    "            output_dir.mkdir(parents=True, exist_ok=True) \n",
    "            molecule_extractor.process_and_save(full_data, output_dir, dataset_name=dataset_name)\n",
    "        else:\n",
    "            print(f\"No 'SMILES' column found in the dataset: {dataset_name}\")\n",
    "    else:\n",
    "        print(f\"Skipping {dataset_name}: Missing one or more of train.csv, val.csv, or test.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667ae798-383f-4013-9283-4b698ad54938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>can_smiles</th>\n",
       "      <th>drug_llm_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OP(O)(=O)C(Cl)(Cl)P(O)(O)=O</td>\n",
       "      <td>O=P(O)(O)C(Cl)(Cl)P(=O)(O)O</td>\n",
       "      <td>[0.598218560218811, 0.6313725113868713, -0.219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC1=NC(=O)N(C=N1)[C@H]1C[C@H](O)[C@@H](CO)O1</td>\n",
       "      <td>Nc1ncn(C2CC(O)C(CO)O2)c(=O)n1</td>\n",
       "      <td>[0.21274812519550323, 0.09204189479351044, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OCCCCCCCCNCO</td>\n",
       "      <td>OCCCCCCCCNCO</td>\n",
       "      <td>[1.0989850759506226, 0.13781903684139252, 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C[C@H](OP(O)(O)=O)[C@@H](N)C(O)=O</td>\n",
       "      <td>CC(OP(=O)(O)O)C(N)C(=O)O</td>\n",
       "      <td>[0.2674747705459595, 0.5900732278823853, -0.20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCO</td>\n",
       "      <td>CCO</td>\n",
       "      <td>[-0.6757968664169312, -1.3385236263275146, -1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         smiles  \\\n",
       "0                   OP(O)(=O)C(Cl)(Cl)P(O)(O)=O   \n",
       "1  NC1=NC(=O)N(C=N1)[C@H]1C[C@H](O)[C@@H](CO)O1   \n",
       "2                                  OCCCCCCCCNCO   \n",
       "3             C[C@H](OP(O)(O)=O)[C@@H](N)C(O)=O   \n",
       "4                                           CCO   \n",
       "\n",
       "                      can_smiles  \\\n",
       "0    O=P(O)(O)C(Cl)(Cl)P(=O)(O)O   \n",
       "1  Nc1ncn(C2CC(O)C(CO)O2)c(=O)n1   \n",
       "2                   OCCCCCCCCNCO   \n",
       "3       CC(OP(=O)(O)O)C(N)C(=O)O   \n",
       "4                            CCO   \n",
       "\n",
       "                                 drug_llm_embeddings  \n",
       "0  [0.598218560218811, 0.6313725113868713, -0.219...  \n",
       "1  [0.21274812519550323, 0.09204189479351044, -0....  \n",
       "2  [1.0989850759506226, 0.13781903684139252, 0.21...  \n",
       "3  [0.2674747705459595, 0.5900732278823853, -0.20...  \n",
       "4  [-0.6757968664169312, -1.3385236263275146, -1....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the generated drug embeddings for the BioSNAP random dataset\n",
    "biosnap_random_drug_embeddings_path = base_dir / f\"embeddings/tda/biosnap_random\"\n",
    "smile_names = np.load(biosnap_random_drug_embeddings_path / f\"drug/biosnap_random_smiles.npy\", allow_pickle=True)\n",
    "can_smile_names = np.load(biosnap_random_drug_embeddings_path / f\"drug/biosnap_random_canonical_smiles.npy\", allow_pickle=True)\n",
    "drug_embeddings = np.load(biosnap_random_drug_embeddings_path / f\"drug/biosnap_random_molecule_embeddings.npy\", allow_pickle=True)\n",
    "biosnap_random_drugs_llm = pd.DataFrame({ 'smiles': smile_names, 'can_smiles': can_smile_names, 'drug_llm_embeddings': drug_embeddings.tolist()})\n",
    "biosnap_random_drugs_llm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba143e-66fd-435f-9648-db88d98594ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
