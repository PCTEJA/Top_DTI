{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dbc88a8-3168-4e88-969a-5a975784ec7e",
   "metadata": {},
   "source": [
    "# Protein Target Embeddings with ProtT5 LLM\n",
    "\n",
    "#### We utilized the <span style=\"color:yellow;\">ProtT5 </span> LLM to extract sequence-based features from protein sequences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b2f472-7bf3-43c9-a0be-3a12bf7b8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ProteinEmbeddingsExtractor:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_name = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name, do_lower_case=False, legacy=True)\n",
    "        self.model = T5EncoderModel.from_pretrained(self.model_name).to(self.device).eval()\n",
    "\n",
    "    #generates embeddings for protein sequences\n",
    "    def get_embeddings(self, seq):\n",
    "        sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq)))]\n",
    "        ids = self.tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "        input_ids = torch.tensor(ids['input_ids']).to(self.device)\n",
    "        attention_mask = torch.tensor(ids['attention_mask']).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding_repr = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        emb_0 = embedding_repr.last_hidden_state[0]\n",
    "        return emb_0.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    def process_and_save(self, data, output_dir, dataset_name):\n",
    "        output_dir = Path(output_dir).resolve()\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #gets unique protein sequences from 'Protein' column of the given dataframe.\n",
    "        unique_sequences = data['Protein'].unique()\n",
    "\n",
    "        sequence_embeddings = []\n",
    "        for seq in tqdm(unique_sequences, desc=f\"Processing sequences in {dataset_name}\"):\n",
    "            embedding = self.get_embeddings(seq)\n",
    "            sequence_embeddings.append(embedding)\n",
    "\n",
    "        embeddings_array = np.array(sequence_embeddings)\n",
    "        np.save(output_dir / f\"{dataset_name}_target_sequences.npy\", unique_sequences)\n",
    "        np.save(output_dir / f\"{dataset_name}_sequence_embeddings.npy\", embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6fafa1-cc2f-4dba-8e63-b768cb8a8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base directory\n",
    "base_dir = Path('Path_to_your_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71dd9e49-aa5a-41e9-9934-a09295478ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_path = base_dir  / 'data'\n",
    "\n",
    "task_paths = {\n",
    "    \"biosnap_random\": data_path / \"biosnap/random\",\n",
    "    \"human_random\": data_path / \"human/random\",\n",
    "    \"human_cold\": data_path / \"human/cold\"}\n",
    "    \n",
    "\n",
    "protein_extractor = ProteinEmbeddingsExtractor()\n",
    "all_datasets = {}\n",
    "\n",
    "# Process each dataset\n",
    "for dataset_name, dataset_path in task_paths.items():\n",
    "    data_dir = Path(dataset_path)\n",
    "\n",
    "   \n",
    "    train_file = data_dir / 'train.csv'\n",
    "    val_file = data_dir / 'val.csv'\n",
    "    test_file = data_dir / 'test.csv'\n",
    "    \n",
    "    if train_file.exists() and val_file.exists() and test_file.exists():\n",
    "        \n",
    "        train_data = pd.read_csv(train_file)\n",
    "        val_data = pd.read_csv(val_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "        \n",
    "      \n",
    "        full_data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "\n",
    "        all_datasets[dataset_name] = full_data\n",
    "\n",
    "        \n",
    "        #output directory to save embeddings\n",
    "        out_dir = base_dir  / 'embeddings' /'llm'/ dataset_name / 'target'\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)   \n",
    "        \n",
    "        protein_extractor.process_and_save(full_data, out_dir, dataset_name=dataset_name)\n",
    "        \n",
    "        print(f\"{dataset_name} dataset loaded successfully. Total rows: {len(full_data)}\")\n",
    "    else:\n",
    "        print(f\"Skipping {dataset_name}: train.csv, val.csv, or test.csv not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0915ce03-2971-47a0-a611-ac3b7af1fb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "      <th>protein_llm_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MGDHAWSFLKDFLAGGVAAAVSKTAVAPIERVKLLLQVQHASKQIS...</td>\n",
       "      <td>[0.040794070810079575, 0.1398317515850067, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MVLDLDLFRVDKGGDPALIRETQEKRFKDPGLVDQLVKADSEWRRC...</td>\n",
       "      <td>[0.07856228947639465, 0.09228259325027466, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MGNLKSVAQEPGPPCGLGLGLGLGLCGKQGPATPAPEPSRAPASLL...</td>\n",
       "      <td>[0.03025723434984684, 0.09058676660060883, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MGNAAAAKKGSEQESVKEFLAKAKEDFLKKWESPAQNTAHLDQFER...</td>\n",
       "      <td>[0.07570360600948334, 0.11278703063726425, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MVNENTRMYIPEENHQGSNYGSPRPAHANMNANAAAGLAPEHIPTP...</td>\n",
       "      <td>[0.07552587240934372, 0.09334281086921692, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sequences  \\\n",
       "0  MGDHAWSFLKDFLAGGVAAAVSKTAVAPIERVKLLLQVQHASKQIS...   \n",
       "1  MVLDLDLFRVDKGGDPALIRETQEKRFKDPGLVDQLVKADSEWRRC...   \n",
       "2  MGNLKSVAQEPGPPCGLGLGLGLGLCGKQGPATPAPEPSRAPASLL...   \n",
       "3  MGNAAAAKKGSEQESVKEFLAKAKEDFLKKWESPAQNTAHLDQFER...   \n",
       "4  MVNENTRMYIPEENHQGSNYGSPRPAHANMNANAAAGLAPEHIPTP...   \n",
       "\n",
       "                              protein_llm_embeddings  \n",
       "0  [0.040794070810079575, 0.1398317515850067, -0....  \n",
       "1  [0.07856228947639465, 0.09228259325027466, 0.0...  \n",
       "2  [0.03025723434984684, 0.09058676660060883, 0.0...  \n",
       "3  [0.07570360600948334, 0.11278703063726425, 0.0...  \n",
       "4  [0.07552587240934372, 0.09334281086921692, 0.0...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the generated target embeddings for the BioSNAP random dataset \n",
    "biosnap_llm_embeddings_path = base_dir / f\"embeddings/llm/biosnap_random\"\n",
    "sequences_names = np.load(biosnap_llm_embeddings_path / f\"target/biosnap_random_target_sequences.npy\", allow_pickle=True)\n",
    "gene_embeddings = np.load(biosnap_llm_embeddings_path / f\"target/biosnap_random_sequence_embeddings.npy\", allow_pickle=True)\n",
    "biosnap_protein_llm = pd.DataFrame({'sequences': sequences_names, 'protein_llm_embeddings': gene_embeddings.tolist() })\n",
    "biosnap_protein_llm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd3d3f-11f7-4fc5-9aae-927bc86ead4d",
   "metadata": {},
   "source": [
    "# Drug Embeddings with MoLFormer LLM\n",
    "\n",
    "### We used the <span style=\"color:yellow;\"> MoLFormer </span> LLM to produce drug representations from chemical SMILES strings.\n",
    "\n",
    "Clone https://github.com/IBM/molformer and change directory to molformer folder\n",
    "\n",
    "MolFormer requires the installation of 'apex.' However, we uninstalled 'apex' after creating drug embeddings due to a compatibility issue with PyTorch.\n",
    "\n",
    "After cloning https://github.com/NVIDIA/apex\n",
    "\n",
    "pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
    "\n",
    "pip install -v --disable-pip-version-check --no-build-isolation --no-cache-dir ./\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acae227c-246e-42b7-9a31-e5327818cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(base_dir /'molformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a854168-737b-484b-8621-d567f7ebcc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import yaml\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "from chem_tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from train_pubchem_light import LightningModule\n",
    "from rdkit import Chem\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a286d24-ce5c-4eff-807d-bbe4bee5424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeEmbeddingsExtractor:\n",
    "    def __init__(self, model_path, checkpoint_path):\n",
    "      \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Loading MolFormer model from: {model_path}\")\n",
    "\n",
    "        # Load configuration\n",
    "        with open(Path(model_path) / 'data/Pretrained MoLFormer/hparams.yaml', 'r') as f:\n",
    "            self.config = Namespace(**yaml.safe_load(f))\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = MolTranBertTokenizer(Path(model_path) / 'bert_vocab.txt')\n",
    "\n",
    "        self.model = LightningModule(self.config, self.tokenizer.vocab).load_from_checkpoint(\n",
    "            Path(checkpoint_path), config=self.config, vocab=self.tokenizer.vocab\n",
    "        ).to(self.device).eval()\n",
    "        \n",
    "\n",
    "    def batch_split(self, data, batch_size=64):  \n",
    "        for i in range(0, len(data), batch_size):\n",
    "            yield data[i:i + batch_size]\n",
    "    \n",
    "    def embed(self, smiles, batch_size=64):\n",
    "        \"\"\"\n",
    "        Embed SMILES strings into molecule embeddings.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        embeddings = []\n",
    "        for batch in self.batch_split(smiles, batch_size=batch_size):\n",
    "            batch_enc = self.tokenizer.batch_encode_plus(batch, padding='longest', add_special_tokens=True)\n",
    "            idx = torch.tensor(batch_enc['input_ids']).to(self.device)\n",
    "            mask = torch.tensor(batch_enc['attention_mask']).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                token_embeddings = self.model.blocks(self.model.tok_emb(idx), length_mask=LM(mask.sum(-1)))\n",
    "\n",
    "            # Average pooling over tokens\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            embedding = sum_embeddings / sum_mask\n",
    "            embeddings.append(embedding.cpu())\n",
    "        return torch.cat(embeddings)\n",
    "\n",
    "  \n",
    "\n",
    "    def canonicalize(self, s):\n",
    "        # Ensure s is not None and is a valid SMILES string\n",
    "        if s is not None and Chem.MolFromSmiles(s):\n",
    "            return Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True, isomericSmiles=False)\n",
    "        else:\n",
    "            return None  # Return None if the SMILES string is invalid or None\n",
    "\n",
    "    \n",
    "    \n",
    "    def process_and_save(self, data, output_dir, dataset_name):\n",
    "         \n",
    "        if 'SMILES' not in data.columns:\n",
    "            raise ValueError(\"Dataset does not contain a 'SMILES' column.\")\n",
    "    \n",
    "        output_dir = Path(output_dir).resolve()\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        # Get unique SMILES\n",
    "        unique_smiles = data['SMILES'].unique()\n",
    "        \n",
    "        # Apply the canonicalization function\n",
    "        canonicalized_smiles = [self.canonicalize(s) for s in unique_smiles]\n",
    "    \n",
    "        print(f\"Extracting embeddings for {len(unique_smiles)} unique SMILES in {dataset_name}\")\n",
    "    \n",
    "        # Filter out invalid canonical SMILES (None)\n",
    "        valid_indices = [i for i, s in enumerate(canonicalized_smiles) if s is not None]\n",
    "        valid_smiles = [unique_smiles[i] for i in valid_indices]\n",
    "        valid_canonical_smiles = [canonicalized_smiles[i] for i in valid_indices]\n",
    "        \n",
    "        # Extract embeddings\n",
    "        if len(valid_canonical_smiles) == 0:\n",
    "            print(\"No valid canonical SMILES found. Skipping embedding extraction.\")\n",
    "            return\n",
    "    \n",
    "        embeddings = self.embed(valid_canonical_smiles).numpy()\n",
    "    \n",
    "        # Double-check: Filter again if embeddings are not generated\n",
    "        if len(valid_canonical_smiles) != len(embeddings):\n",
    "            raise ValueError(\"Mismatch in valid canonical SMILES and embeddings length.\")\n",
    "    \n",
    "        # Save the filtered SMILES, canonical SMILES, and embeddings\n",
    "        np.save(output_dir / f\"{dataset_name}_smiles.npy\", valid_smiles)\n",
    "        np.save(output_dir / f\"{dataset_name}_canonical_smiles.npy\", valid_canonical_smiles)\n",
    "        np.save(output_dir / f\"{dataset_name}_molecule_embeddings.npy\", embeddings)\n",
    "    \n",
    "        print(f\"Saved {len(embeddings)} embeddings.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b95bf66c-cda1-4176-8687-4ca50fec20e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MolFormer model from: /bozdagpool/UNT/mt0994/DTI/molformer\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "\n",
      "Processing dataset: biosnap_random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:46:21] Unusual charge on atom 0 number of radical electrons set to zero\n",
      "[23:46:21] Unusual charge on atom 0 number of radical electrons set to zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for 4505 unique SMILES in biosnap_random\n",
      "Saved 4505 embeddings.\n",
      "\n",
      "Processing dataset: human_random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:28] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for 2726 unique SMILES in human_random\n",
      "Saved 2726 embeddings.\n",
      "\n",
      "Processing dataset: human_cold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:46:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:46:32] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for 1813 unique SMILES in human_cold\n",
      "Saved 1813 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Define model, tokenizer, and checkpoint paths\n",
    "\n",
    "lib_path = Path(base_dir/\"molformer\")\n",
    "checkpoint_path = lib_path / \"data/Pretrained MoLFormer/checkpoints/N-Step-Checkpoint_3_30000.ckpt\"\n",
    "\n",
    "\n",
    "molecule_extractor = MoleculeEmbeddingsExtractor(\n",
    "    model_path=lib_path,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "\n",
    "\n",
    "for dataset_name, dataset_path in task_paths.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    dataset_dir = Path(dataset_path)\n",
    "\n",
    "    train_file = dataset_dir / 'train.csv'\n",
    "    val_file = dataset_dir / 'val.csv'\n",
    "    test_file = dataset_dir / 'test.csv'\n",
    "\n",
    "    if train_file.exists() and val_file.exists() and test_file.exists():\n",
    "       \n",
    "        train_data = pd.read_csv(train_file)\n",
    "        val_data = pd.read_csv(val_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "\n",
    "        full_data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "\n",
    "        # Check for 'SMILES' column and process embeddings\n",
    "        if 'SMILES' in full_data.columns:\n",
    "            output_dir = Path(base_dir /f\"embeddings/tda/{dataset_name}/drug\")\n",
    "            output_dir.mkdir(parents=True, exist_ok=True) \n",
    "            molecule_extractor.process_and_save(full_data, output_dir, dataset_name=dataset_name)\n",
    "        else:\n",
    "            print(f\"No 'SMILES' column found in the dataset: {dataset_name}\")\n",
    "    else:\n",
    "        print(f\"Skipping {dataset_name}: Missing one or more of train.csv, val.csv, or test.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "667ae798-383f-4013-9283-4b698ad54938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>can_smiles</th>\n",
       "      <th>drug_llm_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OP(O)(=O)C(Cl)(Cl)P(O)(O)=O</td>\n",
       "      <td>O=P(O)(O)C(Cl)(Cl)P(=O)(O)O</td>\n",
       "      <td>[0.7787514925003052, 0.34758803248405457, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC1=NC(=O)N(C=N1)[C@H]1C[C@H](O)[C@@H](CO)O1</td>\n",
       "      <td>Nc1ncn(C2CC(O)C(CO)O2)c(=O)n1</td>\n",
       "      <td>[0.21483802795410156, 0.17366445064544678, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OCCCCCCCCNCO</td>\n",
       "      <td>OCCCCCCCCNCO</td>\n",
       "      <td>[0.6010259389877319, -0.36157843470573425, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C[C@H](OP(O)(O)=O)[C@@H](N)C(O)=O</td>\n",
       "      <td>CC(OP(=O)(O)O)C(N)C(=O)O</td>\n",
       "      <td>[0.4546224772930145, 0.39328116178512573, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCO</td>\n",
       "      <td>CCO</td>\n",
       "      <td>[0.9750620126724243, -0.1789940893650055, 0.24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         smiles  \\\n",
       "0                   OP(O)(=O)C(Cl)(Cl)P(O)(O)=O   \n",
       "1  NC1=NC(=O)N(C=N1)[C@H]1C[C@H](O)[C@@H](CO)O1   \n",
       "2                                  OCCCCCCCCNCO   \n",
       "3             C[C@H](OP(O)(O)=O)[C@@H](N)C(O)=O   \n",
       "4                                           CCO   \n",
       "\n",
       "                      can_smiles  \\\n",
       "0    O=P(O)(O)C(Cl)(Cl)P(=O)(O)O   \n",
       "1  Nc1ncn(C2CC(O)C(CO)O2)c(=O)n1   \n",
       "2                   OCCCCCCCCNCO   \n",
       "3       CC(OP(=O)(O)O)C(N)C(=O)O   \n",
       "4                            CCO   \n",
       "\n",
       "                                 drug_llm_embeddings  \n",
       "0  [0.7787514925003052, 0.34758803248405457, -0.1...  \n",
       "1  [0.21483802795410156, 0.17366445064544678, -0....  \n",
       "2  [0.6010259389877319, -0.36157843470573425, -0....  \n",
       "3  [0.4546224772930145, 0.39328116178512573, 0.02...  \n",
       "4  [0.9750620126724243, -0.1789940893650055, 0.24...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the generated drug embeddings for the BioSNAP random dataset\n",
    "biosnap_random_drug_embeddings_path = base_dir / f\"embeddings/tda/biosnap_random\"\n",
    "smile_names = np.load(biosnap_random_drug_embeddings_path / f\"drug/biosnap_random_smiles.npy\", allow_pickle=True)\n",
    "can_smile_names = np.load(biosnap_random_drug_embeddings_path / f\"drug/biosnap_random_canonical_smiles.npy\", allow_pickle=True)\n",
    "drug_embeddings = np.load(biosnap_random_drug_embeddings_path / f\"drug/biosnap_random_molecule_embeddings.npy\", allow_pickle=True)\n",
    "biosnap_random_drugs_llm = pd.DataFrame({ 'smiles': smile_names, 'can_smiles': can_smile_names, 'drug_llm_embeddings': drug_embeddings.tolist()})\n",
    "biosnap_random_drugs_llm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba143e-66fd-435f-9648-db88d98594ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
